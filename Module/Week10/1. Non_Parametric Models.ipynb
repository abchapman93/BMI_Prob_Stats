{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Non-parametric statistics (AKA Distribution Free)\n",
    "\n",
    "We evaluated some non-parametric estimations in previous sections while looking at descriptive statistics (Histograms, Kernel density plots). The advantage of non-paramatric statistics is that they are distribution free or at least the distribution parameters are unspecified.\n",
    "\n",
    "What to do when your data has outliers or does not fit the assumptions of any test statistic:\n",
    "\n",
    "1.  Analyze data with and then without the outlier values and see how outliers affect the results.  These analyses should be discussed in the result and discussion sections.  You cannot ignore outliers!\n",
    "\n",
    "2.  Categorize data and re-analyze.  For example, a measurement of physical activity is the MET – metabolic equivalent of task.  Many people (in one study 53%) score a 0.  Clearly this distribution of the variable will not be close to normal so the usual statistical tests will not be applicable.  But we can categorize MET into none, medium and high levels of activity and use tests that can be used with categorical variables.\n",
    "\n",
    "3.  Use non-parametric methods, a few of which are discussed in this section.\n",
    "\n",
    "4.  Use other tests outside the scope of this course.\n",
    "\n",
    "WHAT ARE NON-PARAMETRIC METHODS?\n",
    "\n",
    "Thus far, we have assumed that X has a known distribution with hypothesized population parameters.\n",
    "\ti.e.  If a quantitative, continuous variable, then our assumptions have been that X~N(μ, σ2) and, therefore, we could use parametric statistical methods.  That is, we assumed that we knew something about the variable (i.e. its distribution) and could use a sample statistic to estimate the population parameter.  The null hypothesis in a parametric test is H0: μ1 =μ2\n",
    " \n",
    "In nonparametric tests, the hypotheses are not about population parameters (e.g., μ=50 or μ1=μ2) and are based on fewer assumptions.  Thus, the hypotheses are more general: are the two populations equal (implying equality in their central tendency.  \n",
    "\n",
    "The cost of fewer assumptions is that non-parametric tests are generally less powerful.  That means that you are more likely to be able to detect a significant effect when one truly exists. \n",
    "\n",
    "The advantage of non-parametric tests is that it may be the only way to analyze some types of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WHEN TO USE NON-PARAMETRIC METHODS:\n",
    "\n",
    "•\tIf the distribution of X is not known (especially if the sample size is small) or the distribution does not 'fit' any of the 'known' distributions such as the distributions below:\n",
    "\n",
    " ![title](Dsitributions.png)\n",
    " \n",
    "•\tWhen the outcome is an ordinal variable or a rank variable.  For example, you are interested in the ability of people to perform independently in six basic activities of daily living with total responses of 0 - 6, where 0 means they are able to perform all six activities to 6 meaning they can perform none of the activities. \n",
    "•\tWhen there are definite outliers.  For example, days spent in a hospital after a surgical procedure.  Most patients fall within a short, specified time, but there will always be some patients who require extra days, and rarely, some may require a very long stay.  These outliers contain important information - they cannot be ignored. \n",
    "\n",
    "•\tWhen the outcome has clear limits of detection.  For example, levels of 1,2,4-trichlorobenzene (1,2,4-TCB) in blood cannot be detected by todays techniques below 0.21 ug/L (https://pubchem.ncbi.nlm.nih.gov/compound/1_2_4-trichlorobenzene).  That does not mean serum levels < 0.21 ug/L, only that it cannot be detected.  Therefore, any analysis of serum levels of 1,2,4-TCB will have an artificial left limit of 0.21 ug/L.\n",
    "\n",
    "•\tWhen the sample size is very small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In parametric methods, the measure of central location used was the mean.  In most non-parametric, or distribution free tests, the measure of central location used is the median.\n",
    "\n",
    "Recall, by definition:\n",
    "\n",
    "\tP(X < M) = .5 and P(X > M) = .5\n",
    "\n",
    "That is, for a quantitative, continuous random variable, the median is defined as the point M such that 50% of the time x lies below M and 50% of the time above M.\n",
    "\n",
    "Many parametric tests have a non-parametric counterpart.  A few non-parametric tests related to what you have learned thus far are:\n",
    "\n",
    "| **Parametric tests (means)** | **Nonparametric tests (medians)** |\n",
    "| --- | --- |\n",
    "| 1-sample t test | 1-sample Sign test |\n",
    "| 2-sample t test | Mann-Whitney test |\n",
    "| 2-sample paired t test | Wilcoxon signed-rank test |\n",
    "\n",
    "For the purposes of this class I only want you to know why, when, and how to use non-parametric methods, not how to calculate the statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contingency tables\n",
    "\n",
    "We saw contingency tables before when calculating probabilities from discrete categorical variables. We can investigate the relationships between the variables with $X^2$ tests \n",
    "\n",
    "Fisher's exact test provides a solution for comparing variables in a 2X2 contingency table for small sample sizes, that does not rely on any distributional assumptions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "THE χ2 DISTRIBUTION\n",
    "\n",
    "Recall, we used the χ2 distribution for the one-sample test of the variance.\n",
    "\n",
    "This same distribution is also used for analyses of **count or frequency data**\n",
    "\n",
    "We will start with the most simple case – our outcome variable of interest is:\n",
    "- dichotomous (yes/no, diseased/not diseased, etc), \n",
    "- we have a fixed n, randomly selected sample, \n",
    "- the probability of success (a yes, diseased, etc) is the same, trial to trial.  \n",
    "\n",
    "That is, we have a variable that is distributed binomially.  Additionally, we want to divided our sample into two groups, say exposed and unexposed.  Thus we are interested in testing $H0: P_{disease|exposed} = P_{disease|unexposed}$.\n",
    "\n",
    "There are to approaches to test the above hypothesis, the normal theory method and the contingency-table method.  The two methods are equivalent (that is the p-values are the same for both methods).  Since the contingency-table method is the most commonly used method, we will concentrated on that and skip the normal theory method.\n",
    "\n",
    "A contingency table is \"A statistical table that shows the observed frequencies of data elements classified according to two variables, with the rows indicating one variable and the columns indicating the other variable\". (American Heritage Dictionary of the English Language, 2009)\n",
    "\n",
    "Since we have two variables (outcome variable and the 'group'  or 'exposure' variable) and each of these variables have two possible values each (diseased/not diseased and exposure/not exposed), we can construct a 2 x 2 contingency table.  We call it a 2 x 2 table because there are two rows and two columns.  This is a subset of the R x C contingency tables (r rows and c columns) we will be discussing later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2 x 2 CONTINGENCY TABLE\n",
    "$H0: P_{disease|exposed} = P_{disease|unexposed}$.\n",
    "\n",
    "Notation: $X_{ij}$ refers to the cell value in the ith row and jth column, e.g. $X_{11}$ is the cell in the 1st row and 1st column, $X_{12}$ is the cell in the 1st row and 2st column, etc\n",
    "\n",
    "For two-sample binomial variable where p = probability of success\n",
    "\n",
    "|   | Success | Failure |​Total |\n",
    "| --- | --- | --- | --- |\n",
    "| Group 1 | x11 | x12 | r1 = x11 + x12 |\n",
    "| Group 2 | x21 | x22 | r2 = x21 + x22 |\n",
    "| Total | c1 = x11 + x21 | c2 = x12 + x22 | N = r1 + r2 = c1 + c2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So the row totals (n1 and n2) are the sum of the 1st and 2nd rows respectively: r1 = x11 + x12  and r2 = x21 + x22  and in the same way, the column totals are the total of the respective cell in those columns.  You will see these cells referred to differently in different books and sometimes within the same book but for different tests.  For example, x11 or n11 or O11, etc.  The margin totals will, however, always be the sum of the cells.  N, the sum of all the observations, is referred to as the grand total – the sum of all the row and column cells.\n",
    "\n",
    "\n",
    "The first statistical test you will learn is the Chi-square (χ2) test, probably the most important and most used non-parametric statistical test.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example 1:  A longitudinal study is conducted to evaluate the long-term complications in diabetic patients treated under two competing treatment regimens.  Complications are measured by evidence of foot disease, eye disease or cardiovascular disease within a 10-year observation period.  Two-hundred people are recruited to take part in the study, 100 people are assigned to Treatment 1 and 100 to Treatment 2.\n",
    "The outcome variable here is whether the subject did or did not have complications as defined above (dichotomous) and the two groups we wish to compare are those treated with either of the two regimens (dichotomous).  The resulting observed tables was:\n",
    "\n",
    "|   | Long - Term Complications |   |\n",
    "| --- | --- | --- |\n",
    "|   | Yes | No | Total |\n",
    "| Treatment 1 | 12 | 88 | 100 |\n",
    "| Treatment 2 | 8 | 92 | 100 |\n",
    "| Total | 20 | 180 | 200 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the above table, x11 = 12, x12 = 88, x21 = 8 and x22 = 92,r1 = 100, c1 = 1, etc, the grand total = 200.  Note, since the numbers in the above tables are the results of what was observed in the cohort, these are often referred to as Oij instead of xij.\n",
    "Thus, an estimate of the population proportion of all patients who develop complications under Treatment 1 (p1) is p ̂_1 = 12/100 = 0.12.  Similarly, an estimate of the population proportion of all patients who develop complications under Treatment 2 (p2) is p ̂_2 = 8/100 = 0.08.  Now, since 0.12 and 0.08 are estimates of the true proportion in the population, it is possible that they both come from the same population.\n",
    "This question leads us to the following:\n",
    "H0:  The probability of a patient developing long-term complications if they receive Treatment 1 is the same as those that received Treatment 2.  {p1 = p2}\n",
    "HA:  The probability of a patient developing long-term complications if they receive Treatment 1 is not the same as those that received Treatment 2.  {p1 ≠ p2}\n",
    "In the above case we are interested in testing p1 = p2.  We would use the χ2 test of homogeneity (the row margins of who received which treatment were fixed) – are the two groups homogeneous in the probability of outcome?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example 2:  Telephone surveys typically have high rates of nonresponse which can cause bias in the analysis results.  It is known that advanced-warning letters letting participants know that a survey is on its way increases overall responses in mail and home surveys. \n",
    "\n",
    "A study to investigate the utility of leaving messages on answering machines as a means of encouraging participation in telephone surveys was added to a telephone survey.  A message was left or not left at random when an answering machine picked up the first call of the telephone survey.\n",
    "\n",
    "A sub-analysis was performed on those calls in which an answering machine picked up the first call.  The outcome here is whether a member of the household completed the survey and the two groups we wish to compare are those that had had a message left on the answering machine to those that did not have a message left on the answering machine.  The resulting observed tables was:\n",
    "\n",
    "|   | Completion of the survey    |\n",
    "| --- | --- | --- |\n",
    "|   | Yes | No | Total |\n",
    "| Message left | 134 | 157 | 291 |\n",
    "| No message left | 33 | 67 | 100 |\n",
    "| Total | 167 | 224 | 391 |\n",
    "\n",
    "In the above table, x11 = 134, x12 = 157, x21 = 33 and x22 = 67, the grand total = 391\n",
    "Thus, an estimate of the population proportion of subjects who would complete a survey when a message was left on their answering machine (p1) is p ̂_1 = 134/291 = 0.46.  Similarly, an estimate of the population proportion of subject who would complete the survey when no message has been left on their answering machine (p2) is p ̂_2 = 33/100 = 0.33.\n",
    "This question leads us to the following:\n",
    "H0:  The probability that a subject will complete a telephone survey if they receive a message on their answering machine is the same as those that did not receive a message.  {p1 = p2}\n",
    "HA:  The probability that a subject will complete a telephone survey if they receive a message on their answering machine is the same as those that did not receive a message.   {p1 ≠ p2}\n",
    "Again, we are interested in testing p1 = p2, only this time we would use the χ2 test of independence (the row margins are the results of randomness) – is completion of the survey independent of the subject having a message left on their answering machine?\n",
    "Luckily, the two tests are exactly the same.  And, in fact, sometimes it is difficult to distinguish between the two tests (homogeneity and independence).  There are two methods used for this test: the Chi-Square Test which approximates the probability of  p1 = p2 and Fisher's Exact Test (FE)  which calculates the exact probability of p1 = p2.  For large numbers, the Chi-square approximation is sufficient, for small number, one should use Fisher's Exact Test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the Chi-Square test.  We wish to compare the **O**bserved contingency table with the **E**xpected tables constructed under the assumption that the null hypothesis is true.\n",
    "Under H0, p1 = p2 = p.  That is, if the null is true, there is a common p that is the same for both groups.\n",
    "\n",
    "That common $ p = \\frac {X_{11} + X_{21}} {N}$\n",
    "\n",
    "Thus E(Success in Treatment 1) = E(x11) = r1p  (recall the expected value of a binomial variable equals np)\n",
    "\n",
    "$$  = r1 \\frac {X_{11} + X_{21}} {n1 + n2}$$\n",
    "\n",
    "$$  = 1^{st} row \\ total \\frac {1^{st} column \\ total} {grand \\ total}$$\n",
    "\n",
    "and in general:\n",
    "\tE(xij) = (ith row total)(jth col total)/grand total\n",
    "\n",
    "So, for example 1 above, we have the observed table:\n",
    "\n",
    "|   | Long - Term Complications |   |\n",
    "| --- | --- | --- |\n",
    "|   | Yes | No | Total |\n",
    "| Treatment 1 | 12 | 88 | 100 |\n",
    "| Treatment 2 | 8 | 92 | 100 |\n",
    "| Total | 20 | 180 | 200 |\n",
    "\n",
    "The expected value under the null hypothesis of p1 = p2 = p for the 1st row, 1st column cell: $E(X_{11}) = 100*20/200 = 10, E(X_{12}) = 100*180/200 = 90$, etc.  The resulting expected table:\n",
    "\n",
    "|   | Long - Term Complications |   |\n",
    "| --- | --- | --- |\n",
    "|   | Yes | No | Total |\n",
    "| Treatment 1 | 10 | 90 | 100 |\n",
    "| Treatment 2 | 10 | 90 | 100 |\n",
    "| Total | 20 | 180 | 200 |\n",
    "\n",
    "**N.B. (Note well) The margin totals remain the same.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the observed to the expected tables using the formula: \n",
    "\n",
    "$$χ^2 = \\sum_i\\sum_j \\frac{(O_{ij}- E_{ij})^2}{E_{ij}} $$\n",
    "\n",
    "where the χ2 distribution for a 2 x 2 table has 1 degree of freedom:\n",
    "O11 = 12, O12 = 88, O21 = 8, O22 = 92 and E11 = 10, E12 = 90, E21 = 10, E22 = 90\n",
    "\n",
    "Therefore  \n",
    "\n",
    "$$χ^21 = \\frac{(12-10)^2}{10}+\\frac{(88-90)^2}{90}+\\frac{(8-10)^2}{10}+\\frac{(92-90)^2}{90}$$ \n",
    "\n",
    "= 0.40 + 0.04 + 0.40 + 0.04 = 0.88\n",
    "\n",
    "![title](Ex.1X2.png)\n",
    "\n",
    "Since 0.88 lies in the do not reject region we conclude that we do not have enough evidence to say that the probability of complications for those people who received Treatment 1 is different than the probability of complications for those who received Treatment 2. Neither treatment has an advantage over the other with respect to preventing further complications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = matrix(c(12,88,8,93),ncol=2,byrow=TRUE)\n",
    "\n",
    "\n",
    "chisq.test(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to note:\n",
    "\n",
    "1.  The chi-square test is a two-sided test even though the critical region is one sided.  That is because the observed can be less than the expected or it can be greater than the expected; in either case (O-E)2 will always be positive.  The larger the difference between the observed and the expected → the larger the value numerator → the large the χ2 → the less likely that p1 = p2. \n",
    "\n",
    "2.  As stated above, the chi-square test yields a p-value that is an approximation of the true p-value. You will notice that your book demonstrates using a continuity correction.  That is, the numerator of the chi-square test is ((O-E)-0.5)2 instead of  (O-E)2.  The correction (-0.5) is usually a conservative estimate of chi-square, because the value of chi-square is adjusted downward. There is considerable disagreement about whether or not Yate's correction is too conservative or not.  Most of the time you will see the chi-square test without Yate's continuity correction. \n",
    "\n",
    "\n",
    "\n",
    "Yates' correction modifies the χ2 statistic for a 2×2 contingency table in an effort to correct the error made by using a (continuous) χ2 distribution to approximate the (discrete) sampling distribution of the statistic.\n",
    "\n",
    "## Run the Second example in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do when the numbers are small?  The chi-square approximation is not good for small numbers so we us another test.  At this point, you need to know that there is, yet again, another way to refer to the cells of a 2 x 2 table:\n",
    "\n",
    "|a|\tb |\n",
    "|---|---|\n",
    "|c|\td |\n",
    "\n",
    "The x11 is referred to as the a cell, x12 as the b cell etc.  The x11 notation is generic and can be used to any size table, a,b,c and d are used only for 2 x 2 tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FISHER’S EXACT TEST (FE) calculates the exact probability for any 2 X 2 table. If any cell has Eij < 5 then Fisher’s exact test is the test to use; if all cells have Eij > 5 then either the X2 test or the Fisher’s exact test is appropriate. When calculating the FE, we assume the margin totals are fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_{(a,b,c,d)} = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{n!a!b!c!d!}$$\n",
    "\n",
    "To calculate the p-value of the test, one first calculates the p-values of all possible tables with the same row and column margins as the observed table.  Note, tables are noted by the 'a' cell, so the first table is denoted as p(0) because the a cell value is 0.\n",
    "\n",
    "![title](tables.png)\n",
    "\n",
    "p-value:\t0.1667\t0.5000\t0.3000\t0.0333  (applying the formula to each table)\n",
    "\n",
    "For a one-sided (lower) test, add the probability of the observed table plus the probabilities of any tables to the left of the observed table:  p(0) + p(1) = 0.1667 + 0.5000 = 0.6667\n",
    "\n",
    "For a one-sided (upper) test, add the probabilities of the observed table plus the probabilities of any tables to the right of the observed table:  p(1) + p(2) + p(3) = 0.5000 + 0.3000 + 0.0333 = 0.8333\n",
    "\n",
    "For a two-sided test, add the probability of all tables whose probabilities are less than or equal to the observed table (this is the method used by many statistical packages): the observed table has a probability of 0.50, all other tables have probabilities less than the observed, so we would add all the probabilities p(1) + p(2) + p(3) + p(4) = 1.000\n",
    "\n",
    "You will never have to calculate the values of the FE test by hand, but you should know how it is calculated and when to use it.  When any of the expected cell sizes are > 5, the p-values of the chi-square test and the FE test are very close.  Since large cell values would mean there are many tables that could exist with the same margin totals, and we would need to calculate an exact p-value for each table, using the chi-square test is recommended when calculating by hand.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ej.df = matrix(c(1,2,3,4), nrow = 2)\n",
    "fisher.test(ej.df)\n",
    "#fisher.test(ej.df, alternative = \"less\")\n",
    "#fisher.test(ej.df, alternative = \"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2. \n",
    "Consider a trial comparing the performance of two challengers. Each challengers undertook the trial eight times and the number of successful trials was recorded. The hypothesis under investigation in this experiment is that the performance of the two challengers is similar.\n",
    "\n",
    "||Succeful|Not Succesful|\n",
    "|---|---|---|\n",
    "|Challenger 1|1|\t7 |\n",
    "|Challenger 2|4|\t4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqs = matrix(c(1, 7, 4, 4), ncol = 2, byrow = TRUE)\n",
    "rownames(freqs) = c(\"Challenger 1\", \"Challenger 2\")\n",
    "colnames(freqs) = c(\"Succesful\", \"Not succesful\")\n",
    "challenge <- as.table(freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher.test(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAIRED ANALYSIS\n",
    "\n",
    "The last test using a 2 x 2 table that we will study, is the special case where we do not have independent groups, but ones that are paired (or matched).  As with the t-test, (remember the paired t-test?), we must use a special test to analyze paired data.\n",
    "\n",
    "### Example\n",
    "A survey was constructed to test whether Experiencing Joint Pain is associated with running more than 25km/week, comparing whether someone experiences joint pain before and after some treatment.\n",
    "\n",
    "H0:  There is no association between joint pain and running more than 25 km/ week after applying some treatment.\n",
    "\n",
    "HA:  There is an association between joint pain and running more than 25 km/ week after applying some treatment.\n",
    "\n",
    "\n",
    "TWO-SAMPLE TEST FOR BINOMIAL PROPORTIONS FOR MATCHED-PAIR DATA (McNEMAR’S TEST)\n",
    "\n",
    "The McNemar is not testing for independence, but consistency in responses across two variables.\n",
    "\n",
    "|   |   | Experienced joint pain after treatment |   |\n",
    "| --- | --- | --- | --- |\n",
    "|   |   | Yes | No | Total |\n",
    "| Experienced joint pain before treatment | Yes | 215 | 75 | 290 |\n",
    "| |No | 785 | 380 | 1165 |\n",
    "| |Total |  1000 | 455 | 1455 |   |\n",
    "\n",
    "IF $b + c \\geq 20:  X^2 = \\frac{(\\mid b-c\\mid-1)^2}{b+c}$ with 1 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ((785-75)-1)^2\n",
    "b = 75+785\n",
    "X2 = a/b \n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the critical value $X_{(1,0.95)}^2 = 584.51$.  Since 584.51 >> 3.84, the p-value must be << 0.05,  we reject the null hypothesis that there is no association between joint pain and running more than 25 km/ week after applying some treatment and conclude that these is evidence to say there is an association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pain = matrix(c(215, 75, 785, 380), ncol = 2, byrow = TRUE)\n",
    "mcnemar.test(pain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ODDS RATIO (OR)\n",
    "\n",
    "Before we leave the 2 x 2 table, there is a very common measure of effect used in many disciplines we should look at called the odds ratio (OR).\n",
    "\n",
    "First, odds are defined as $\\frac {p}{(1-p)}$ where p is the probability of the event.  So, if the probability of getting a cold is 0.40 then the odds of getting a cold $= \\frac {0.40}{(1-0.40)}=  \\frac {0.40}{0.60}$  or 2 to 3; if the probability equals 0.25 then the odds are 1 to 3, if the probability is .75 then the odds are 3 to 1, etc. \n",
    "\n",
    "A common measure of effect in case-control studies is the odds – the odds of exposure among the diseased, and the odds of exposure among the not diseased.  If we take the ratio of these two odds, we have the …. **odds ratio:**\n",
    "\n",
    "$$\\frac {P_{d|e}/(1-P_{d|e})}{P_{d|e} /(1-P_{d|e})}$$\n",
    "\n",
    "\n",
    "To calculate the odds ratio: construct a 2x2 contingency table with either cases in the first row and the presence of the risk factor in the first column (or the presence of the risk factor in the first row and cases in the first column).  You will see both scenarios in papers so you must learn it as the odds and not just as a, b, c and d.\n",
    "\n",
    "||Case|Control|\n",
    "|---|---|---|\n",
    "|Exposed|a|\tb |\n",
    "|Unexposed|c|d|\n",
    "\n",
    "Let $p_{case|exp}$ = probability of being a case among the exposed.  Then $p_{case|exp} = \\frac {a}{(a+b)}$\n",
    "\t\n",
    "then the odds of being a case among the exposed = $\\frac {a/(a+b)}{1-[a/(a+b)]} = \\frac {a}{b} $\n",
    " \n",
    "Let $p_{case|no-exp}$ = probability of being a case among the non-exposure.  Then $p_{case|no-exp} =\\frac  {c}{(c+d)} $\n",
    "\n",
    "and the odds of being case among the non-exposed = $\\frac {c/(c+d)}{1-[c/(c+d)]} = \\frac {c}{d} $\n",
    "\n",
    "Thus, the odds ratio of $\\frac {(odds \\ of \\ being \\ a  \\ case \\ among\\  the\\ exposed)}{(odds\\ of\\ being\\ a\\ case\\ among\\ the\\ unexposed)} = \\frac {a/b}{c/d} = \\frac {ad}{bc}$           \n",
    "\n",
    "\n",
    "One method (Woolf) for calculating the CI95 for the OR is:\n",
    "\n",
    "$\\large e^{ln⁡(\\hat{OR})±z_{1-∝/2} \\sqrt{(\\frac {1}{a} + \\frac {1}{b}+\\frac {1}{c}+\\frac {1}{d})}}\\large$\n",
    "\n",
    "Now, if the OR = 1 then the odds of disease among the exposed = odds of disease among the non-exposed which is equivalent to saying 'exposure does not appear to play a role in the disease.'  If the OR > 1 then exposed people are more likely to be diseased than those not exposed so it would appear that exposure is a risk.  If the OR < 1 then exposed people are less likely to be diseased than those that are unexposed, therefore the exposure has a protective effect (think medical treatment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "A clinical trial of gamma globulin in the treatment of children with Kawasaki syndrome (a rare but fatal condition) randomized approximately half of the patients to receive gamma globulin.  The other half received the standard treatment of aspirin.  Under the aspirin treatment, approximately one fourth of patients developed coronary abnormalities.  It was thought that gamma globulin would help prevent the development of coronary abnormalities.  Subjects were followed over a 7-week period.\n",
    "\n",
    "H0:  Patients receiving gamma globulin will develop coronary abnormalities at the same or higher rate as those patients receiving the present standard of care which is aspirin. (OR ≥ 1)\n",
    "\n",
    "Ha:  Patients receiving gamma globulin will develop fewer coronary abnormalities then those patients receiving the present standard of care which is aspirin. (OR < 1)\n",
    "\n",
    "\n",
    "The results were:\n",
    "\n",
    "|||Coronary abnormalities|Total|\n",
    "| --- | --- | --- | --- |\n",
    "||Yes|No||\n",
    "|Gamma globulin|5|\t78 |83|\n",
    "|Aspirin |21|\t63 |84|\n",
    "|Total|26|141|167|\n",
    "\n",
    "This is a 'case/control' scenario because Kawasaki syndrome is, fortunately, rare.  Thus, we cannot use the relative risk as a measure of effect but will use the odds ratio instead. \n",
    "\n",
    "$$\\hat{OR}  =  \\frac {5*63}{78*21}=0.19$$\n",
    "\n",
    "The CI for the OR: $= exp(ln(0.19) ± 1.96\\sqrt {\\frac{1}{5}+ \\frac {1}{78}+ \\frac {1}{21} + \\frac {1}{63}} = $ \n",
    "\n",
    "exp(-1.66 ± 1.03) = (0.0679, 0.5326).  Since the CI does not contain the null hypothesis (OR = 1), we would reject the null hypothesis and conclude that gamma globulin appears to protect Kawasaki patients from ancillary coronary abnormalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#install.packages(\"vcd\")\n",
    "library(vcd)\n",
    "tabl1 = matrix(c(5, 78, 21, 63), ncol = 2, byrow = TRUE)\n",
    "odds_rat = oddsratio(tabl1, log = FALSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(odds_rat)\n",
    "confint(odds_rat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(\"CoalMiners\")\n",
    "CoalMiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Odd_Coal = oddsratio(CoalMiners)\n",
    "woolf_test(CoalMiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Odd_Coal)\n",
    "confint(Odd_Coal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
