{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference Statistics\n",
    "\n",
    "The scientific method\n",
    "\n",
    "![title](Scientific_Method_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem\n",
    "\n",
    "As we have seen previously, many of the distributions we have seen (discrete and continuous) when we increase our sample size, sufficiently, the mean of all samples drawn from the population will be approximately equal to the mean of the population.\n",
    "\n",
    "These samples will follow an approximate normal distribution, with the variances approximatelly equal to the variance of the population divided by the sample's size.\n",
    "\n",
    "In many instances a sufficiently large sample is generalized to a size larger than 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Review an example that will allows to introduce our next topic Point Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Lets generate some data that represent the whole population age distribution\n",
    "##install.packages(\"e1071\")\n",
    "library(e1071)\n",
    "set.seed(12)\n",
    "population_ages <- c(rexp(1000000,0.015)+18,   # Generate a population\n",
    "                     rpois(500000,20)+18,\n",
    "                     rpois(500000,32.5)+18,\n",
    "                     rpois(500000,45)+18)\n",
    "\n",
    "population_ages <- ifelse(population_ages<100, population_ages, population_ages%%100+18)\n",
    "\n",
    "true_mean <- mean(population_ages)\n",
    "true_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(10)\n",
    "sample_ages <- sample(population_ages, size=1000)  # Take a sample of 1000 ages\n",
    "\n",
    "sample_mean <- mean(sample_ages)            # Make a point estimate of the mean\n",
    "\n",
    "sample_mean\n",
    "\n",
    "sample_mean-true_mean   # Check difference between estimate and population parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(12)\n",
    "population_races <- c(rep(\"white\",1000000),    # Generate some dummy demographic data\n",
    "                      rep(\"hispanic\",500000),\n",
    "                      rep(\"black\",500000),\n",
    "                      rep(\"asian\",250000),\n",
    "                      rep(\"other\",250000))\n",
    "\n",
    "demographic_sample <- sample(population_races, size=1000)       # Take a sample\n",
    "\n",
    "for (race in unique(demographic_sample)){            # Loop over each race*\n",
    "  print(paste(race,\" proportion estimate:\"))       \n",
    "  print(sum(demographic_sample==race)/1000)        # Print the estimated proportion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist(population_ages, breaks=20)  # Create histogram of population\n",
    "\n",
    "skewness(population_ages)         # Check the skewness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist(sample_ages, breaks=20)   # Create histogram of the sample\n",
    "\n",
    "skewness(sample_ages)          # Check the skewness (point estimate of skewness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set.seed(12)\n",
    "point_estimates <- c()    # Create an empty vector to hold results\n",
    "\n",
    "num_samples <- 200        # Initialize number of samples to take\n",
    "\n",
    "for (x in 1:num_samples){         # Draw 200 samples and make 200 point estimates\n",
    "  sample <- sample(population_ages, size=1000)\n",
    "  point_estimates <- c(point_estimates, mean(sample))\n",
    "}\n",
    "\n",
    "plot(density(point_estimates))  # Plot the sampling distribution\n",
    "skewness(point_estimates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean(point_estimates)\n",
    "\n",
    "mean(point_estimates)-true_mean    # Difference between true mean and sample means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "\n",
    "A point estimate can give you a rough idea of a population parameter like the mean, but estimates are prone to error and taking multiple samples to get improved estimates may not be feasible. \n",
    "\n",
    "A confidence interval is a range of values above and below a point estimate that captures the true population parameter at some predetermined confidence level. For example, if you want to have a 95% chance of capturing the true population parameter with a point estimate and a corresponding confidence interval, you'd set your confidence level to 95%. Higher confidence levels result in a wider confidence intervals.\n",
    "\n",
    "\n",
    "Calculate a confidence interval by taking a point estimate and then adding and subtracting a margin of error to create a range. Margin of error is based on your desired confidence level, the spread of the data and the size of your sample. The way you calculate the margin of error depends on whether you know the standard deviation of the population or not.\n",
    "\n",
    "\n",
    "If you know the standard deviation of the population, the margin of error is equal to:\n",
    "\n",
    "$$ z * \\frac {\\sigma}{\\sqrt{\\bar{n}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#install.packages(\"mosaic\")\n",
    "library(mosaic)\n",
    "\n",
    "trellis.par.set(theme=col.mosaic())\n",
    "options(digits=3)\n",
    "\n",
    "###\n",
    "mu = 500\n",
    "sigma = 100\n",
    "x = rnorm(500, mean=mu, sd=sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##produces a complete set of stats from a vector\n",
    "favstats(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#introducing a kernel density estimator\n",
    "plot(density(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Estimating confidence intervals\n",
    "\n",
    "meanconfint = function (x, sigma, level = 0.95, ...) {\n",
    "  se = sigma / sqrt(length(x))\n",
    "  mu = mean(x)\n",
    "  z = qnorm(1 - (1 - level)/2)\n",
    "  out = c(mu, mu - z * se, mu + z * se)\n",
    "  names(out) = c(\"mean\", \"lower\", \"upper\")\n",
    "  return(out)\n",
    "}\n",
    "\n",
    "meanconfint(x, sigma = sigma)\n",
    "\n",
    "z_critical <- qnorm(0.975)\n",
    "z_critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###do function from the mosaic package, repear something many times\n",
    "randomx = do(50) * rnorm(500, mean=mu, sd=sigma)\n",
    "\n",
    "ci = data.frame(t(apply(randomx, 1, meanconfint, sigma=sigma)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###plot confidence intervals -- sometimes the \n",
    "### simulation it doesn't cover the actual mean\n",
    "xyplot(1:nrow(ci) ~ mean, data=ci, xlim=range(ci), xlab=\"SAT score\", ylab=\"Index\")\n",
    "ladd(panel.abline(v=500, col=\"lightgray\", lty=2))\n",
    "ladd(with(ci, panel.arrows(x0 = lower, y0=1:nrow(ci), y1=1:nrow(ci), cex=0.5,\n",
    "                             x1=upper, code=3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The action of resampling with replacement is called boostrapping (We will come back to this method)\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = c(190.5, 109, 95.5, 137)\n",
    "resample(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bootstrap = do(1000) * mean(resample(time))\n",
    "densityplot(~mean, data=bootstrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Once we have collected our data based on our initial hypothesis, is time to start exploring relationships across variables and they type of data that we have gathered.\n",
    "\n",
    "We saw that point estimators allows us to have a general idea of the type of data and tha ranges where the data operates.\n",
    "\n",
    "Next week, we will review multiple methods that allows us to detect possible error in the data and explore types of estimators that will help us with our analysis.\n",
    "\n",
    "The central idea of hypothesis testing is to mathematically test whether our expectations are approximatelly close to the real population parameters. \n",
    "\n",
    "The classical approach is to generate intrinsic hypothesis which will allow to compartamentalize the decision regarding the validity of the approach.\n",
    "\n",
    "### $H_o$ *null hypothesis* It is normally the default value - usually is the condition that is false\n",
    "\n",
    "### $H_1$ *Alternative hypothese* It is normally the hypothesis we want to test (the one that is true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We want to be able to determine with some percentage of confidence (normally it is 95) if we can reject the null hypothesis, in favor of the Alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Is the coin biased?? Hypothesis testing\n",
    "library(mosaic)\n",
    "n_tosses = 1000\n",
    "\n",
    "lower_bound = qbinom(0.025, n_tosses, 0.5)\n",
    "upper_bound = qbinom(0.975, n_tosses, 0.5)\n",
    "\n",
    "\n",
    "lower_bound\n",
    "upper_bound\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coin_flip = function(){\n",
    "  ifelse(runif(1)>0.51,1,0)\n",
    "}\n",
    "\n",
    "coin_flip_biased = function(){\n",
    "  rbinom(1,1,0.7)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observed_head_count = sum(do(n_tosses) * coin_flip())\n",
    "#observed_head_count = sum(do(n_tosses) * coin_flip_biased())\n",
    "\n",
    "if (observed_head_count >= lower_bound & observed_head_count <= upper_bound){\n",
    "  print(\"Failed to reject the null!\")\n",
    "}else{\n",
    "  print(\"Null rejected!\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formally, hypothesis testing can be expressed: if X is a random variable and R is the range of X. $R_{reject} \\subset $ R is the rejection region. If $X \\in R_{reject}$ then the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = rnorm(1000,0,1)\n",
    "y = rnorm(1000,2,1)\n",
    "\n",
    "dx = density(x)\n",
    "dy = density(y)\n",
    "\n",
    "plot(dx, xlim = c(-8,12))\n",
    "lines(dy, col = \"forestgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = rnorm(1000,0,0.5)\n",
    "y = rnorm(1000,0,2)\n",
    "\n",
    "dx = density(x)\n",
    "dy = density(y)\n",
    "\n",
    "plot(dx, xlim = c(-8,12))\n",
    "lines(dy, col = \"forestgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
